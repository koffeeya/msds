---
title: "Untitled"
author: "Kavya Beheraj and Jeremy O'Brien""
date: "April 28, 2018"
output: html_document
---

**TO DO: RMD formatting**

```{r setup, include = F}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, message = F, warning = F}

# TO DO: conditional install: if (!require('package')) install.packages('package')
library(dplyr)
library(magrittr)
library(stringr)
library(tidyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(devtools)
# devtools::install_github("nicolewhite/RNeo4j")
library(RNeo4j)
library(recommenderlab)
```

```{r setup, echo = F}

# Clean up environment
rm(list = ls())

```

<br>

<hr>

<br>

# Overview

**TO DO: [Summarize overview in slide presentation]**

**TO DO: [Complete overview last]**

**TO DO: [Resolve whether rmd include shiny, calls separately, or requires user behavior to run: https://rdrr.io/cran/rmarkdown/man/run.html]**

<br>

## Background

The Million Song Database, or MSD (https://labrosa.ee.columbia.edu/millionsong/) is an open source dataset of one million popular songs made freely available by The Echo Nest (http://the.echonest.com/).

The Echo Nest is a music intelligence and data platform to understand the audio and textual content of recorded music acquired.  It was spun off from MIT Media Lab around 2005, and acquired by Spotify in 2014, to power playlist curation (https://en.wikipedia.org/wiki/The_Echo_Nest).   
In addition to a wealth of other song metadata, the dataset also charts the intersection of unique listeners with specific songs, thus providing a view into listening habits.

The MSD data was collected using The Echo Nest API and musicbrainz, the open music encyclopedia (https://musicbrainz.org/).  The MSD FAQ indicates that data was "downloaded during December 2010", but does not provide detail on the span of time, platforms, or geographies over which data was collected.  This presents some constraints on treating the dataset as a representative sample of listening behavior or song popularity.

The full dataset is available via AWS and has been mirrored by the Open Science Data Cloud.  

**TO DO: [Add reference to where we found our subset]**

<br>

## Approach

The MSD provides a count of song listens by unique (hashed) user IDs.  These song listens can be treated as implicit "ratings" of a song.  Interpreting this at scale can provide an interesting picture of aggregate song popularity.  Examining these implicit "ratings"" at level of listeners can provide an indication of individual preference, both by the songs they include and do not include, as well as the proportion of total listens a given song represent for that listener.

NB: the MSD does not define whether listens represent song starts, track-time completes, or some other measure, an important consideration when evaluating listening behavior.

We'd like to evaluate whether a simple metric of song listens can prove a robust indicator of listeners' song preferences and help us predict - based on a set of song preferences - other songs to recommend which a given listeners would like.

To restate explicitly, our hypothesis is that the proportion of times a listener listens to a given song compared with total listens of all other songs represents a useful signal of preference.

**TO DO: [Elaborate on steps and confirm they tick all requirements in project brief**

* We evaluate this by analyzing the MSD and constructing...
* We build a graph database from our dataset using Neo4j (our first time ever!), interfacing with R through RNeo4J.  This is an elegant approach to representing the relationships between listeners and the songs they listen to.  As this is a proof-of-concept (POC), we are not hosting this database on the web and require local installation of Neo4j.
* We also explored RecommenderLab (appendix) as an alternative approach, but concentrated our efforts on Neo4j based on early success...
* We build a similarity matrix
* We train the recommender model in Neo4j 
* We test the recommender model, evaluating its performance on predicted song rating vs. 
* We build a simple UI in Shiny (another maide voyage for us!) to make it easier to interact with the recommender.  Again, as this is a POC, we are not hosting the Shiny app on the web so the code in this RMD must be run in order to demonstrate the Shiny UX.

<br>

<hr>

<br>

# Read in the data

**TO DO: [Explain how we collected the dataset, and what it includes / does not - i.e. not the 300GB maste file, obviously]**

**TO DO: [Clean up column naming taxonomy]**

```{r}

# Read in the ratings dataframe and rename the columns
u1 <- "https://static.turi.com/datasets/millionsong/10000.txt"
df1 <- as.data.frame(read.table(u1, header = F, stringsAsFactors = F))
names(df1) <- c("user_id", "song_id", "listen_count")

# Read in the metadata dataframe
u2 <- "https://static.turi.com/datasets/millionsong/song_data.csv"
metadata <- as.data.frame(read.csv(u2, header = T, sep = ",", stringsAsFactors = F))

```

**TO DO: [join these code chunks?]

```{r}

# Join data by song ID. Remove duplicate song ratings.
joined <- distinct(inner_join(df1, metadata, by = "song_id"))

# Group and summarize joined dataframe by user ID
grouped_id <- joined %>%
  select(user_id, listen_count) %>%
  group_by(user_id) %>%
  summarise(number_songs = n(), 
            mean_listen_count = mean(listen_count), 
            sum_listen_count = sum(listen_count))

grouped_song <- joined %>% 
  select(song_id, title, artist_name) %>% 
  group_by(title)

nrow(grouped_song)

```

**TO DO: [Can we strike this?]**

```{r}

View(grouped_song)

```

<br>

<hr>

<br>

# Summarize the data

MSD is a large dataset, so to better understand it we perform some EDA on the dataframes, including summarization and visualization.
* Grouped_id is a listener-level summary: total number of listens, total number of songs, and average listens per song, all for a given listener (user ID).
* Joined is a detailed set of tidied observations keying song to listener: song title, artist, release, year, and a unique song ID; along with count of listens by listener.

<br>

## Listener-level summary statistics**

```{r, echo = F}

# Calculate high-level statistics on listeners
nrow(grouped_id)
summary(grouped_id$sum_listen_count)
summary(grouped_id$mean_listen_count)

```

<br>

**TO DO: [Requires more interpretation]**

Some listener-level summary statistics: the MSD include 76,353 individuals, each of whom has to at least one song (obvious, but a good sense check).  On average, individuals have listened to 3.183 songs.  The most songs any individual has listened to is 192.  


```{r}

# TO DO: [Plot theses against one another, par(mfrow = c(1, 3)) for GGPLOT?]

ggplot(data = grouped_id, aes(number_songs)) + 
  geom_histogram(binwidth = 1) +
  labs(title = "How people listen: songs vs. listeners", x = "Unique songs", y = "Total listeners")

ggplot(data = grouped_id, aes(number_songs)) + 
  geom_histogram(breaks = seq(1, 100, by = 1)) +
  labs(title = "How people listen: songs vs. listeners", subtitle = "<100 songs (Detail)", x = "Unique songs", y = "Total listeners")

# TO DO: calculate cume peak of histogram

max(grouped_id$number_songs)
mean(grouped_id$number_songs)

```

<br>

**TO DO: [Describe curve - songs listened on x, number of individuals at the level on y.  Power with long tail.  Peak between 8 and 16 songs listened to (CONFIRM)]**

```{r}

# TO DO: parse meaning of this plot

ggplot(data = grouped_id, aes(x = number_songs, y = sum_listen_count)) +
         geom_point() +
         geom_smooth(method = "loess", se = F) +
         xlim(c(0, 800)) +
         ylim(c(0, 4000))
# labs: title, subtitle, caption, x, y

# TO DO: [Plot histograms of mean listens.  Describe]

# TO DO: [Add description of box / whisker, consider whether to look into quantiles using mutate(quintile = ntile(mean_listen_count, 5) or mean_listen_count]

ggplot(data = grouped_id, aes(x = "", y = number_songs)) +
  geom_boxplot(varwidth = T)
# labs: title, subtitle, caption, x, y

```

<hr>

## Song-level summary statistics

**TO DO: [Quick note here]**

```{r}

length(unique(joined$song_id)) # number of unique songs

min(joined$year[which(joined$year > 0)]) # earliest recording (correcting for null values coded as 0)

max(joined$year[which(joined$year > 0)]) # latest recording (correcting for null values coded as 0

# TO DO: [Disentangle the following.]

sum(joined$listen_count) # total number of listens?
summarise(df1, total_listens = sum(listen_count)) # total number of listens?

summary(joined$listen_count) # number of times a song was listened to, on average

sd(joined$listen_count)

# TO DO: [Analyze whether songs that get lots of listens have lots of listeners and calcuclate mean for that subset.]

joined %>% 
  select(user_id, song_id, listen_count) %>% 
  group_by(song_id) %>% 
  summarise(total_listens = sum(listen_count), unique_listeners = n_distinct(user_id)) %>%
  ggplot(aes(x = total_listens, y = unique_listeners)) +
           geom_point()

# TO DO: [Additional summary stats]

```

The MSD is true to its name and includes a million songs..

**TO DO: [Summarize stats above]**

<br>

<hr>

<br>

# Calculate user ratings

**TO DO: [Insert description]**

```{r}

# Filter the IDs by users who have rated 15 or more songs
filtered_id <- filter(grouped_id, number_songs>=15)

# Join total listen count to the full dataframe.
joined2 <- left_join(joined, filtered_id, by = "user_id")

# Create a new column to hold a calculated implicit rating (as a percentage from 0 to 100) of user preference for a song. 
joined_final <- mutate(joined2, rating = round(((joined2$listen_count / joined2$sum_listen_count)*100), 2))

# Filter out single song listeners, identifying them by their 100% rating.
joined_final <- filter(joined_final, rating<1)

```

<br>

<hr>

<br>

# Sample the data

Now that we have cleaned data, we can prepare it for modeling by taking a random sample.

```{r}

# Create a dataframe of unique user IDs. There are 75,491 users in the cleaned dataframe joined_final.
user_list <- distinct(as.data.frame(joined_final$user_id))
names(user_list) <- c("user_id")
n <- nrow(user_list)

s3_user <- sample(user_list$user_id, round(n*0.005), replace = F)
names(s3_user) <- c("user_id")
s3 <- distinct(subset(joined_final, joined_final$user_id %in% s3_user))
s3 <- as.data.frame(select(s3, user_id, song_id, rating, title, release, artist_name))

print(sprintf('The entire dataset contains %d users.', n))
print(sprintf('The sample contains %d users.', round(n*0.005)))

```

<br>

<hr>

<br>

# Build graph

**TO DO: [Insert description]**

**TO DO: [Explain that Neo4j desktop must be installed and running a local (empty) database (with PW?) in order to implement code, which is okay because this is POC]

```{r include=FALSE}

pw = "dbpassword"

```

```{r}

# Initialize graph locally.  Open Neo4j desktop to enable and start graph to proceed.
graph = startGraph("http://localhost:7474/db/data/", username="neo4j", password=pw)

```

```{r}

# TO DO: [Insert description]
q1 <- "
MERGE (user:User {id: {user_id}}) 
MERGE (song:Song {song_id: {song_id}, title: {title}, artist: {artist}, album: {album}}) 
CREATE (user)-[r:RATED {rating: {rating}}]->(song)
SET r.rating = TOFLOAT({rating})
"

tx <- newTransaction(graph)

for (i in 1:nrow(s3)) {
  row <- s3[i , ]
  appendCypher(tx, q1,
               user_id = row$user_id,
               song_id = row$song_id,
               title = row$title,
               rating = row$rating,
               artist = row$artist_name,
               album = row$release)
}

commit(tx)

summary(graph)

```

```{r}

# TO DO: [Insert description]
q2 <- 
"MATCH (p1:User)-[x:RATED]->(s:Song)<-[y:RATED]-(p2:User)
WITH SUM(x.rating * y.rating) AS xyDotProduct,
 SQRT(REDUCE(xDot = 0.0, a IN COLLECT(x.rating) | xDot + a^2)) AS xLength,
 SQRT(REDUCE(yDot = 0.0, b IN COLLECT(y.rating) | yDot + b^2)) AS yLength,
 p1, p2

MERGE (p1)-[s:SIMILARITY]-(p2)

SET s.similarity = xyDotProduct / (xLength * yLength)"

cypher(graph, q2)

```

**TO DO: [Update next section**]

```{r}

# Choose a random user ID: f5ff17f21882c64331348c3c71ceba4575102625

#r <- sample(filtered_id$user_id, 1)

q5 <- "

MATCH (b:User)-[r:RATED]->(m:Song), (b)-[s:SIMILARITY]-(a:User {id:'7d7cbc47d6a956967e593f62c9ec6549724e9b47'})
WHERE NOT((a)-[:RATED]->(m))

WITH m, s.similarity AS similarity, r.rating AS rating
ORDER BY m.title, similarity DESC

WITH m.song_id AS song_id, COLLECT(rating)[0..3] AS ratings

WITH song_id, REDUCE(s = 0, i IN ratings | s + i)*1.0 / LENGTH(ratings) AS reco
ORDER BY reco DESC
RETURN song_id AS song_id, reco AS recommendation

"

f <- cypher(graph, q5)

g <- inner_join(grouped_song, f, by="song_id") %>%
   arrange(desc(recommendation)) %>% 
   distinct()

g

View(g)

```

<br>

<hr>

<br>

# EVALUATE RECOMMENDER PERFORMANCE

**TO DO: [Described evaluation of performance]**

<br>

<hr>

<br>

# Create Shiny UI as proof-of-concept

TO DO: [Update and edit this section]

Our Shiny implementation should have three elements.  While these elements are web-enabled, for the purposes of this project we are not building a web-accessible database, so neo4j will need to be installed and open locally in order to run the recommender.

1) The first element is intended to elicit user ratings of three songs in the MSD graph - presumably (though no necessarily) songs they like.  This information is used to find similar users in the graph.  It is output to a temporary dataframe, so that the user name and song can be added to the neo4j graph as additional nodes / relationships.
* A bar in which to enter the user's name, which is used to key the node 
* Three "search bars" for users to type the name of each song
* An adjacent slider on which to rank each song on a 1-10 scale
* When users type a song, the "search bar"dynamically queries the graph
* Optimally the search bar autofills so users don't go to the trouble of entering a song not recorded in the graph
* As a fallback, the search bar includes a search button users will need to push to query the database and return results in a clickable list of songs and corresponding artists
* In either case, if users enter a song not recorded, some sort of warning indicates no results found, and to choose another song

2) The second element is a a list of recommended songs yielded by user-user similarity measures mapped in the MSD graph.  This information serves is the output of the recommender system.  The MSD graph returns a temporary dataframe of top-N songs i.e. those highly rated by similar users.  This is based on the three songs the user proferred
* This list of songs and corresponding artists (provided for reference) is visualized as a simple table.
* As a feature improvement, this list could click to a search engine to provide users an opportunity to listen to the recommended songs or learn more about the recommended artists.

3) The third element is a "restart" button.
* This refreshes the first and second elements so a user can restart the recommender.

**TO DO: [Insert Shiny code block or external app call here]**

<br>

<hr>

<br>

# Test UX and demo

**TO DO: [We'll need to create some test cases (i.e. song lookups) that we know work for a demo, as our data subsets won't call the full OMD (right?).]**

**TO DO: [Include RMD-friendly example of recommender / UI performance?]**

<br>

<hr>

<br>

# Conclusions

**TO DO: [Add conclusions / findings and next steps for iteration of the recommender / UI]**

<br>

<hr>

<br>

# Appendix: RecommenderLab

**TO DO: [Clean up and include only needed portions]**

## Background
RecommenderLab (RL) is based on collaborative filtering approaches.
Collaborative filtering takes items ratings produced by users.
Uses those as the basis to predict ratings for other users; or create top-N recos for an active user
If its memory-based, it does so on the whole dataset; if model-based, it learns a more compact model like preference-clusters users that makes recos.  Not sure if substantive difference for our purposes?

## Structuring data
Uses a rating matrix with rows as users and items as columns.
In our case, rows are listeners and songs are columns.
This leads to a very wide matrix that is horizontally sparse.
Not clear if a tidy implementation can be ingested by RL?
Rating scale?  Require logistic regression?
What are signals for ratings we should keep out?
Does it require and adjusted cosine similarity score as input, is that in the black box?

Do we need a user-item matrix?  Used for CF
Do we need a similarity matrix?  Used for item-item CF

## Steps 
Create input matrix
Normalize using normalize()?  Depends on data prep, transforms, etc. 
Convert to binaries using binarize()?  Again, depends on data prep, etc.
Check distro 
getRatings() to extract vector with non-missing ratings
hist(getRatings(normalize(r, method = "Z-score")), breaks = 100)
hist(rowCounts(r), breaks = 50)
hist(colMeans(r), breaks= 20)

## Evaluating prediction performance

For rating prediction, Mean Average Error or Root Mean Square Error (penalizes larger errors stronger)

For Top-N recos, create confusion matrix
Evaluate accuracy through correct recos / total possible recos
Mean absolute error or mean absolute deviation

To evaluate information retrieval performance, user precision and recall
Precision = true pos / (true pos + false pos)
Recall = true pos / (true pos + false neg)
Precision mapped to y
Recall to x
E-measure = 1 / (alpha(1 / precision) + (1 - alpha(1 / recall)))

```{r, eval=FALSE, message=FALSE}

# Will start with sampled data to test out structure.

head(sampled)
str(sampled)
rownames(sampled)

# The first step is before feeding recommenderlab models is to create a sparse matrix.  On our first pass, we'll treat listen_count as an implicit rating to test.  We'll try to leverage approach described here: https://rpubs.com/tarashnot/recommender_comparison

# sparse_sampled <- sparseMatrix(i = sampled$user_id, j = sampled$song_id, x = sampled$listen_count, dims = c(length(unique(sampled$user_id)), length(unique(sampled$song_id))), dimnames = list(paste("u", 1:length(unique(sampled$user_id)), sep = ""), paste("m", 1:length(unique(sampled$song_id)), sep = "")))

# This throws an error (non-numeric argument to binary operator).  Looks like we'll need to convert user_id and song_id to integers.  For users, since rows are equivalent to users we can use rownames and coerce to numeric.  For songs, we'll try to create a song_id2 field that creates an integer based on unique values in the song_id field.  We'll implement just for sampled df before trying on the joined df.

sampled2 <- transform(sampled, user_id2 = as.numeric(factor(user_id)))
sampled3 <- transform(sampled2, song_id2 = as.numeric(factor(song_id)))
sampled.columns <- c("user_id", "user_id2", "song_id", "song_id2", "listen_count", "title", "release", "artist_name", "year")
sampled3 <- sampled3[, sampled.columns]

sampled3.test1 <- sampled3[order(sampled3$user_id2),]
sampled3.test2 <- sampled3[order(sampled3$song_id2),]
head(sampled3.test1) # kludgy, but looks like the user_id2 approach worked
head(sampled3.test2) # kludgy, but looks like the song_id2 approach worked

# We'll work with sampled3 dataframe below

sparse_sampled <- sparseMatrix(i = sampled3$user_id2, j = sampled3$song_id2, x = sampled3$listen_count, dims = c(length(unique(sampled3$user_id2)), length(unique(sampled3$song_id2))), dimnames = list(paste("u", 1:length(unique(sampled3$user_id2)), sep = ""), paste("m", 1:length(unique(sampled3$song_id2)), sep = "")))

# We'll create a test matrix to feed the recommender

reco_feed <- as(sparse_sampled, "realRatingMatrix")

# Here's a sample view
# getRatingMatrix(reco_feed[c(1:5, c(1:4))])

# We'll create a recommender for the 80 users included in the sampled set

r.popular <- Recommender(reco_feed[1:80], method = "POPULAR")

top5reco <- predict(r.popular, reco_feed[81:100], n = 5)

as(top5reco, "list")[1:3]

```
